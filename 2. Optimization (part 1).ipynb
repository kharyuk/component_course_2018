{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Курс \"Компонентные модели\"\n",
    "\n",
    "## Автор: Харюк Павел, аспирант факультета ВМК МГУ имени М.В. Ломоносова\n",
    "### Составлено: 2017-2018 гг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Занятие 2. Задача нелинейных наименьших квадратов и методы оптимизации (часть 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вводная часть\n",
    "\n",
    "На прошлом занятии мы наметили основное направление данного курса. Задача нелинейных наименьших квадратов играет одну из главных ролей в курсе - именно она используетс как базовая составляющая всех рассматриваемых здесь моделей. \n",
    "Занятия, посвящённые методам оптимизации, во многом опираются на курс Ника Гоулда [\"Непрерывная оптимизация\"](http://www.numerical.rl.ac.uk/people/nimg/course/lectures/raphael/lectures/courseoutline.pdf) (Оксфорд):\n",
    "http://www.numerical.rl.ac.uk/people/nimg/course/lectures/raphael/lectures/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нелинейные наименьшие квадраты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача нелинейных наименьших квадратов в вещественном случае формулируется следующим образом:\n",
    "\n",
    "$$\\min\\limits_{z} \\frac{1}{2}\\|F(z) - X\\|_F^2$$\n",
    "\n",
    "\n",
    "$$z \\in \\mathbb{R}^{N}, \\quad F_i: \\mathbb{R}^{N} \\to \\mathbb{R}^{M}, \\quad F(z) = \\begin{bmatrix} F_1(z) \\\\ F_2(z) \\\\ \\vdots \\\\ F_M(z) \\\\ \\end{bmatrix} $$\n",
    "\n",
    "***Замечание.*** Если, по аналогии, рассмотреть $G_i(z) = F_i(z) - X_i$, то задачу можно переписать в виде\n",
    "$$\\min\\limits_{z} \\frac{1}{2} \\|G(z)\\|_F^2, \\quad  g(z) = \\frac{1}{2} \\|G(z)\\|_F^2$$\n",
    "\n",
    "Задача нелинейных наименьших квадратов отличается особым видом градиента и матрицы Гессе. Для их записи удобно использовать матрицу Якоби отображения $f$:\n",
    "$$J_F(z) = J_G(z) = \\begin{bmatrix}\n",
    "\\frac{\\partial F_1(z)}{\\partial z_1} & \\ldots & \\frac{\\partial F_1(z)}{\\partial z_N} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial F_M(z)}{\\partial z_1} & \\ldots & \\frac{\\partial F_M(z)}{\\partial z_N} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Градиент (вектор-столбец первых производных) имеет вид:\n",
    "\n",
    "$$\\nabla G(z) = J_f^T(z) \\, G(z)$$\n",
    "\n",
    "Матрица Гессе (матрица вторых производных) записывается как :\n",
    "\n",
    "$$H_G(z) = J_f^T(z) J_f(z) + \\sum\\limits_{i=1}^{M} G_i(z) \\cdot \\nabla^2 G_i(z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизация функционалов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среди оптимизационных задач принято различать *безусловные задачи*, в которых переменная $z$ может принимать любое значение из $\\mathbb{R}^n$ (напомним, мы рассматриваем оптимизацию параметров из вещественного пространства), и *безусловные задачи*, где $z \\in Q \\subset \\mathbb{R}^n$. Остановимся на безусловной задаче поиска минимума.\n",
    "\n",
    "***Замечание.*** Задача поиска максимума для некоторого функционала $f(z)$ эквивалентна поиску минимума фугнкционала $-f(z)$.\n",
    "\n",
    "Из теории оптимизации известны условия оптимальности:\n",
    "\n",
    "**1. Необходимое условие первого порядка**: Если отображение $f: \\mathbb{R}^n \\to \\mathbb{R}$ дифференцируемо в точке $z^* \\in \\mathbb{R}^n$ и имеет локальный минимум в ней, то $\\nabla f(z^*) = 0$ (то есть, является стационарной точкой).\n",
    "\n",
    "**2. Необходимое условие второго порядка**: Если отображение $f: \\mathbb{R}^n \\to \\mathbb{R}$ дважды дифференцируемо в точке $z^* \\in \\mathbb{R}^n$ и имеет локальный минимум в ней, то гессиан $H_f(z^*)$ неотрицательно определён, $(H_f(z^*) h, \\, h) \\geq 0$, $\\forall h \\in \\mathbb{R}^n$\n",
    "\n",
    "**3. Достаточное условие**: Если отображение $f: \\mathbb{R}^n \\to \\mathbb{R}$ дважды дифференцируемо в точке $z^* \\in \\mathbb{R}^n$, причём $\\nabla f(z^*) = 0$, а гессиан в ней положительно определён, $(H_f(z^*) h, \\, h) > 0$, $\\forall h \\neq 0$, то $z^*$ - точка локального минимума отображения.\n",
    "\n",
    "Обратим внимание, что в сформулированных условиях оптимальности речь идёт лишь о *локальном оптимуме*. При этом важно не путать локальность и глобальность сходимости и оптимальности. Локальность минимума означает существование некоторого шара радиуса $\\varepsilon$ $B(\\varepsilon, z^*)$, для любых точек которого значение функционала $f(z)$ не меньше, чем $f(z^*)$. Глобальный минимум, как понятно из названия, означает, что значение функционала $f(z)$ не меньше, чем $f(z^*)$ уже для любых значений $z \\in \\mathbb{R}^n$.\n",
    "\n",
    "Сходимость же относится к итерационным методам поиска локального или глобального минимума. Глобальная сходимость означает, что рассматриваемый метод сходится к, вообще говоря, локальному минимуму для любого начального приближения. Локальная сходимость же говорит о том, что найдутся такие начальные приближения, для которых метод не сходится ни к одному локальному минимуму\n",
    "\n",
    "Существуют специальные классы задач, для которых можно гарантировать нахождение локального минимума. Например, выпуклые задачи, для которых локальный минимум существует, единствинен и совпадает с глобальным. Кроме того, в случае безусловной оптимизации на всём пространстве $\\mathbb{R}^n$ для выпуклого функционала существует и единственна точка глобального минимума, которая одновременно является стационарной (то есть, градиент функционала в этой точке равен нулю).\n",
    "\n",
    "**Определение.** Функционал $f: \\mathbb{R}^n \\to \\mathbb{R}$ называется выпуклым тогда и только тогда, когда $f(\\lambda x + (1- \\lambda) y) \\leq \\lambda f(x) + (1- \\lambda) f(y)$, $\\forall x, y \\in \\mathbb{R}^n$, $\\forall \\lambda \\in [0, 1]$.\n",
    "\n",
    "Рассматриваемые далее методы можно рассматривать как приближение исходного функционала некоторым модельным функционалом, который имеет более простую структуру. Если исходная задача выглядит как $\\min\\limits_{z} f(z)$, причём функционал дважды дифференцируем, то выписав ряд Тейлора с тремя первыми членами:\n",
    "\n",
    "$$ f(z) = f(z_0) + \\big( \\nabla f(z), z - z_0 \\big) + \\frac{1}{2} \\big( H_f(z) (z - z_0), z- z_0\\big) + \\widehat{o} (\\|z-z_0\\|^2),$$\n",
    "\n",
    "модельную функцию можно выбрать, отбросив остаточный член:\n",
    "\n",
    "$$m(z) = f(z_0) + \\big( \\nabla f(z), z - z_0 \\big) + \\frac{1}{2} \\big( H_f(z) (z - z_0), z- z_0\\big)$$\n",
    "\n",
    "При использовании методов первого порядка отбрасывается слагаемое, включающее матрицу Гессе.\n",
    "\n",
    "Одним из классов методов построения приближённого решения оптимизационных задач являются итерационные методы (методы последовательных приближений). В них строится последовательность точек, которая сходится к решению задачи. Каждое следующее приближение использует информацию, полученную на предыдущих шагах.\n",
    "\n",
    "Так ли плохи локальные минимумы? Осторожный ответ - \"зависит от задачи\". Во всяком случае, в задачах многомерной оптимизации возникает другая проблема - седловые точки:\n",
    "https://arxiv.org/abs/1406.2572\n",
    "\n",
    "Седловые точки образованы множеством стационарных точек за вычетом из них точек локальных экстремумов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из классических методов оптимизации является метод градиентного спуска. Для дифференцируемого функционала направление градиента совпадает с направлением роста значения функционала, а антиградиента - с направлением уменьшения его значения. Таким образом, оптимизационный метод принимает следующий вид:\n",
    "\n",
    "$$x_{k+1} = x_k - \\alpha_k \\nabla f(x_k), \\quad \\alpha_k > 0$$\n",
    "\n",
    "Для того, чтобы использовать метод, необходимо задать значение шага $\\alpha_k$. Оно может быть постоянным, изменяться с ростом числа итераций, либо вычисляться исходя из других соображений.\n",
    "\n",
    "Метод сложно использовать для задач многомерной оптимизации из-за того, что метод не умеет избегать седловые точки. Кроме того, метод обладает линейной скоростью сходимости:\n",
    "\n",
    "$$\\| x_{k+1} - x^* \\| \\leq q \\| x_{k} - x^* \\|$$\n",
    "\n",
    "Это означает, что на каждой итерации появляется $\\log_{10} q$ правильных знаков приближения точки оптимума, $x^*$.\n",
    "\n",
    "Однако, для некоторых задач есть возможность относительно дешёвого увеличения скорости сходимости метода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Акселерация Андерсона"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод акселерации Андресона позволяет повысить скорость сходимости методов неподвижной точкиж именно в виде поиска неподвижной точки некоторого отображения и задаются итерационные методы: $x_{k+1} = g(x_k)$. Метод Андерсона использует ранее вычисленнные приближения для поиска следующего:\n",
    "\n",
    "$$x_{k+1} = (1- \\beta_k) \\sum\\limits_{i=0}^{m_k} \\alpha_i^{(k)} x_{k-m_k + i} + \\beta_k \\sum\\limits_{i=0}^{m_k} \\alpha_i^{(k)} g(x_{k-m_k + i}), \\quad m_k = \\min(m, k), \\quad \\sum\\limits_{i=0}^{m_k} \\alpha_i = 1, \\quad \\beta_k \\in [0, 1]$$\n",
    "\n",
    "Параметр $\\beta_k$ является релаксационным параметром (подробнее см. в [x], [xx]); для простоты будем использовать постоянное значение $\\beta_k = 1$. араметры $\\{ \\alpha_i\\}_{i=0}^{m_k}$ ищутся на каждой итерации метода как неизвестные для задачи\n",
    "\n",
    "$$\\begin{array}{rl}\n",
    "& \\min\\limits_{\\{ \\alpha_i\\}_{i=0}^{m_k}} \n",
    "\\Big\\| \\begin{bmatrix} g(x_{k-m_k}) - x_{k-m_k} \\big| & \\ldots & \\big| g(x_k) - x_k\\end{bmatrix} \n",
    "\\begin{bmatrix} \\alpha_0 \\\\ \\vdots \\\\ \\alpha_{m_k} \\end{bmatrix} \\Big\\|^2_2 = \\min\\limits_{\\{ \\alpha_i\\}_{i=0}^{m_k}} \\big\\| \\alpha_0 \\big(g(x_{k-m_k}) - x_{k-m_k}\\big) + \\ldots + \\alpha_{m_k} \\big(g(x_{k}) - x_{k}\\big) \\big\\|_2^2 \\\\\n",
    "s.t. & \\sum\\limits_{i=0}^{m_k} \\alpha_i = 1 \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "Обратим внимание, что это задача условной оптимизации. Известно, что такую задачу можно свести к безусловной задаче наименьших квадратов:\n",
    "\n",
    "$$\\min\\limits_{\\{ \\theta_i\\}_{i=1}^{m_k}} \\big\\| \\big(g(x_{k}) - x_{k}\\big) + \\sum\\limits_{i=1}^{m_k} \\theta_i \\big( (g(x_{k-i}) - x_{k-i}) - (g(x_{k}) - x_{k}) \\big) \\big\\|_2^2, \\quad \\theta_0 = 1 - \\sum\\limits_{i=1}^{m_k} \\theta_i$$.\n",
    "\n",
    "Для практической реализации более удобна следующая форма задачи:\n",
    "\n",
    "$$\\min\\limits_{\\{ \\theta_i\\}_{i=0}^{m_k}} \\big\\| \\big(g(x_{k}) - x_{k}\\big) + \\sum\\limits_{i=0}^{m_k-1} \\theta_i \\big( (g(x_{k-i+1}) - x_{k-i+1}) - (g(x_{k-i}) - x_{k-i}) \\big) \\big\\|_2^2, \\quad\n",
    "\\alpha_i = \\begin{cases} \\theta_0, & i=0\\\\ \\theta_{i+1}-\\theta_i, & 1 \\leq i < m_k \\\\ 1-\\theta_{m_k-1}, & i = m_k \\\\\\end{cases}$$.\n",
    "\n",
    "По сути, здесь решается задача линейной регрессии на набор векторов.\n",
    "\n",
    "Анализ сходимости метода Андерсона приведён в [pp], здесь же укажем лишь сам результат: если у отображения $g(x)$ существует неподвижная точка, начальное приближение достаточно близко к ней, $g(x)$ дифференцируемо в этой окрестности и Липшицево с константой $L \\in (0, 1)$, тогда найдётся такое $\\widehat{L} \\in (L, 1)$, что:\n",
    "$$\\|g(x_k) - x_k\\|_2 \\leq \\widehat{L}^k \\|g(x_0) - x_0\\|_2 $$\n",
    "\n",
    "Далее будет рассмотрена пара методов для решения систем линейных уравнений с эрмитовой матрицей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод сопряжённых градиентов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если рассматривается задача $Ax = b$, $A = A^* > 0$, то её решение можно искать через оптимизацию функционала энергии:\n",
    "\n",
    "$$J(x) = \\frac{1}{2} \\big( Ax, x\\big) - \\big(b, x \\big)$$\n",
    "\n",
    "Пусть $x_0$ и $x^*$ - начальное приближение и точное решение задачи. Первый шаг метода происходит в направлении антиградиента, который совпадает с невязкой $r = b - A x$. Метод построен таким образом, чтобы использовать текущую невязку и информацию ото всех предыдущих направлений. Кроме того, каждое следующее направление должно быть $A$-ортогонально всем предыдущим (два вектора $u, v$ $A$-ортогональны, если $(Au, v) = 0$): это освободит нас от хранения всех предыдущих направлений и позволит сэкономить память.\n",
    "\n",
    "Из требований к методу получаем следующее правило для обновления направления:\n",
    "\n",
    "$$p_k = r_{k-1} - \\Big(\\frac{(A p_1, r_{k-1})}{(A p_1, p_1)}p_1 + \\ldots + \\frac{(A p_{k-1}, r_{k-1})}{(A p_{k-1}, p_{k-1})} p_{k-1}\\Big)$$\n",
    "\n",
    "При этом в пространстве $\\mathbb{R}^n$ вектора $\\{p_1, \\ldots, p_n\\}$ образуют базис, и это означает, что за $n$ шагов решение будет построено:\n",
    "\n",
    "$$x^* = x_0 + \\alpha_1 p_1 + \\ldots + \\alpha_n p_n, \\quad x_{k+1} = x_k + \\alpha_k p_k$$\n",
    "\n",
    "Из разложения точного решения можно получить выражение для определения шага:\n",
    "\n",
    "$$\\alpha_k = \\frac{(r_k, p_k)}{(A p_k, p_k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для произвольного функционала $f(x)$ обновления по методу CG ведутся в следующем виде:\n",
    "\n",
    "$$x_{k+1} = \n",
    "\\begin{cases}\n",
    "    x_k - \\alpha_k \\nabla f(x_k), & k = 0 \\\\\n",
    "    x_k + \\alpha_k (- \\nabla f(x_k) + \\beta_k d_k ), & k > 0 \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "$d_k$ - направление на предыдущем шаге.\n",
    "\n",
    "Выбор $\\alpha_k$ происходит как в методе наискорейшего спуска, $\\alpha_k = \\arg \\min_{\\alpha} f(x_k + \\alpha d_k)$, для $\\beta$ существует несколько вариантов:\n",
    "\n",
    "$$\\begin{array}{l}\n",
    "    (\\beta_{\\text{FR}})_k = \\frac{(\\nabla f(x_k), \\nabla f(x_k))}{(\\nabla f(x_{k-1}), \\nabla f(x_{k-1}))}\\\\\n",
    "    (\\beta_{\\text{PR}})_k = \\frac{(\\nabla f(x_k), \\nabla f(x_k) - \\nabla f(x_{k-1}))}{(\\nabla f(x_{k-1}), \\nabla f(x_{k-1}))} \\\\\n",
    "    (\\beta_{\\text{HS}})_k = - \\frac{(\\nabla f(x_{k}), \\nabla f(x_{k}) - f(x_{k-1}))}{(d_{k-1}, \\nabla f(x_k) - \\nabla f(x_{k-1}))} \\\\\n",
    "    (\\beta_{\\text{DY}})_k = \\frac{(\\nabla f(x_{k}), \\nabla f(x_{k}))}{(d_{k-1}, \\nabla f(x_k) - \\nabla f(x_{k-1}))} \\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод сопряжённых невязок\n",
    "\n",
    "В случае, если решается задача $Ax = b$ с эрмитовой, но не обязательно положительно определённой матрицей, и предпочтителен метод со структурой, схожей с методом сопряжённых градиентов, можно использовать метод сопряжённых невязок.\n",
    "\n",
    "$$x_{k+1} = x_k + \\alpha_k (r_k - \\beta_k p_k)$$\n",
    "\n",
    "Использованы обозначения: $r_k = b - A x_k$ - невязка, $p_k$ - вспомогательные векторы, $\\alpha_k$ - шаг метода, $\\beta_k$ - вспомогательный числовой параметр. Вычисляются параметры следующим образом:\n",
    "\n",
    "$$r_k = \\begin{cases} b - A x_0, & k = 0 \\\\ r_{k-1} - \\alpha_{k-1} A p_{k-1}, & k > 0\\end{cases}, \\quad\n",
    "p_k = \\begin{cases} r_0, & k = 0 \\\\ r_k + \\beta_{k-1} p_{k-1}, & k > 0\\end{cases}, \\quad\n",
    "\\alpha_k = \\frac{\\big( A r_k, r_k\\big)}{\\big( A p_k, A p_k\\big)}, \\quad\n",
    "\\beta_k = \\frac{\\big( A r_{k+1}, r_{k+1}\\big)}{\\big( A r_k, r_k\\big)}$$\n",
    "\n",
    "Также полезна формула обновления значения выражения $A p_k$, которая позволяет сэкономить одну операцию умножения матрицы на вектор:\n",
    "\n",
    "$$A p_k = A r_k + \\beta_{k-1} A p_{k-1}$$\n",
    "\n",
    "Методы сопряжённых градиентов и сопряжённых невязок могут быть полезны для реализации методов второго порядка, где возникает задача обращения матрицы Гессе или её аппроксимации.\n",
    "\n",
    "(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Методы второго порядка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы второго порядка опираются на матрицу Гессе (матрицу вторых производных) функуионала, либо на её аппроксимацию. Отправная точка методов - метод Ньютона, который в одномерном случае имеет вид:\n",
    "\n",
    "$$f(x) = 0, \\quad x_{k+1} = x_k - \\frac{f(x_k)}{f^{'} (x_k)}$$\n",
    "\n",
    "Многомерное расширение требует работы с матрицей якоби отображения $F(x)$:\n",
    "\n",
    "$$F(x) = 0, \\quad x_{k+1} = x_k - J_F^{-1}[x_k] F(x_k)$$\n",
    "\n",
    "Откуда получаются именно такие формулы? Вспомним, что метод Ньютона имеет второе название, метод касательных. Рассмотрим подробнее уравнение касательной $f(x_k) + f^{'}(x_k) (x_{k} - x) = 0$: левая часть совпадает с первыми двумя членами ряда Тейлора. Отсюда заключаем, что метод опирается на замену исходной функции её модельной аппроксимацией, полученной из ряда Тейлора.\n",
    "\n",
    "Как использовать метод для произвольного многомерного функционала? Из необходимого условия оптимальности следует, что в точках локального минимума градиент функционала равен нулевому вектору (уточним только, что этим же свойством обладают и седловые точки); а это и есть искомое отображение $F(x) = 0$:\n",
    "$$\\nabla f(x) = 0, \\quad \\nabla f(x) \\approx \\nabla f(x_{k}) + \\nabla^2 f(x_{k}) (x - x_k), \\quad \\nabla^2 f(x_{k}) (x - x_k) = - \\nabla f(x_{k}) $$\n",
    "\n",
    "$\\nabla^2 f(x_{k})$ - матрица Гессе для функционала $f(x)$. Далее мы будем обозначать её как $H_k$. Видно, что поиск обновления требует решения системы линейных уравнений; для этого может быть использован один из методов, рассмотренных ранее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аппроксимация матрицы Гессе: обновления ранга 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далеко не всегда удаётся вычислить точную матрицу Гессе. В таких случаях используют разные её аппроксимации. Один из способов - обновления ранга 1. Идея в том, чтобы использовать аппроксимации матрицы, вычисленные на предыдущих шагах итерационного метода. Ранг-1 обновление - значит, что к предыдущей аппроксимации будет прибавлена матрица ранга 1, для задания которой необходимо задать один вектор $u_k$:\n",
    "$$\\widehat{H}_{k+1} = \\widehat{H}_k + u_k u_k^T$$\n",
    "\n",
    "Каким образом выбирать $u_k$? Во-первых, предъявим такое требование: использовать как можно полнее те велечины, которые и так придётся вычислять, к ним относятся, в частности, градиент функционала. Далее, рассмотрим модельный функционал для исходной задачи при текущем приближении решения $x_k$:\n",
    "\n",
    "$$m_k(x) = f(x_k) + \\big( \\nabla f(x_k), x - x_k \\big) + \\frac{1}{2} \\big( \\widehat{H}_k (x - x_k), (x - x_k) \\big) $$\n",
    "\n",
    "Разумное требование - чтобы новая аппроксимация матрицы Гессе соответствовала изменению градиента при переходе в новую точку:\n",
    "\n",
    "$$\\nabla f(x_{k+1}) - \\nabla f(x_k) = \\nabla m(x_{k+1}) - \\nabla m(x_k) = \\widehat{H}_{k+1} \\Delta x_k $$\n",
    "\n",
    "Такое требование называют условием секущих:\n",
    "\n",
    "$$\\widehat{H}_{k+1} \\cdot \\Delta x_k = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$$\n",
    "\n",
    "Распишем $\\widehat{H}_{k+1}$:\n",
    "\n",
    "$$\\widehat{H}_{k} \\cdot \\Delta x_k + (\\Delta x_k, u_k) u_k = \\nabla f(x_{k+1}) - \\nabla f(x_{k}), \\quad u_k = \\frac{\\nabla f(x_{k+1}) - \\nabla f(x_{k}) - \\widehat{H}_{k} \\Delta x_k}{(\\Delta x_k, u_k)}$$\n",
    "\n",
    "Выражение в знаменателе легко получить, умножив скалярно расписанное условие секущих на $\\Delta x_k$:\n",
    "$$(\\Delta x_k, u_k)^2 = (\\nabla f(x_{k+1}) - \\nabla f(x_{k}) - \\widehat{H}_{k} \\Delta x_k, \\Delta x_k)$$\n",
    "\n",
    "Недостаток метода: нет гарантии, что аппроксимация матрицы Гессе будет положительно определённой. Следующий метод обходит этот недостаток."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS метод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом методе используется обновление ранга 2:\n",
    "\n",
    "$$\\widehat{H}_{k+1} = \\widehat{H}_{k} + uu^T + vv^T, \\quad {dim}\\, \\mathcal{L} (u, v) = 2$$\n",
    "\n",
    "$$\\widehat{H}_{k+1} = \\widehat{H}_{k} - \\frac{B_k \\Delta x_k (B_k \\Delta x_k)^T }{(B_k \\Delta x_k, \\Delta x_k)}  + \\frac{(\\nabla f(x_{k+1}) - \\nabla f(x_{k})) (\\nabla f(x_{k+1}) - \\nabla f(x_{k}))^T}{(\\nabla f(x_{k+1}) - \\nabla f(x_{k}), \\Delta x_k)}$$\n",
    "\n",
    "В обоих случаях возникает проблема решения системы линейных уравнений. Можно решать их приближённо с помощью итерационных методов, а можно воспользоваться формулой Шерманна-Мориссона-Вудберри:\n",
    "\n",
    "$$\\begin{bmatrix} A & B \\\\ C & D \\\\ \\end{bmatrix} $$\n",
    "\n",
    "$$(A + BDC)^{-1} = A^{-1} - A^{-1} B( D^{-1} + C A^{-1} B)^{-1} C A^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уравнение $H_{k+1} \\Delta x_k = -\\nabla f(x_k)$ заменяется системойуравнений с помощью разложения Холецкого:\n",
    "\n",
    "$$H_{k+1} = L_k L_k^T, \\quad \n",
    "\\begin{cases}\n",
    "    L_k g_k = -\\nabla f(x_k) \\\\\n",
    "    L_k^T \\Delta x_k = g_k \\\n",
    "\\end{cases}$$\n",
    "\n",
    "Вместо поиска правила обновления $H_{k}$ в методе ищется обновление нижней треугольной матрицы $L_k$.\n",
    "\n",
    "Теперь, пусть $H_k = B_k B_k^T$, где $B_k$ - произвольная матрица. Задача обновления формулируется в следующем виде:\n",
    "\n",
    "$$\\min_B \\|B - B_k\\|_F\\\\\n",
    "s.t. \\, B g_k = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$$\n",
    "\n",
    "После решения этой задачи шаг выбирается из условия $B_{k+1} \\Delta x_k = g_k$.\n",
    "\n",
    "Как решается минимизационная задача? Во-первых, её можно переформулировать в следующем виде:\n",
    "\n",
    "$$\\min_B (B - B_k, B - B_k)\\\\\n",
    "s.t. \\, (B, e_i g_k^T) = (\\nabla f(x_{k+1}) - \\nabla f(x_{k}))_i, \\quad i=\\overline{1, n}$$\n",
    "\n",
    "Задачу можно решить используя метод множителей Лагранжа (см. материалы занятия 4 и стандартные курсы оптимизации), откуда получим систему уравнений:\n",
    "$$\\begin{cases}\n",
    "2(B - B_k) - \\lambda g_k^T = 0 \\\\\n",
    "B g_k = \\nabla f(x_{k+1}) - \\nabla f(x_{k})\n",
    "\\end{cases},$$\n",
    "которая имеет решение $B_{k+1} = B_k + \\frac{(\\nabla f(x_{k+1}) - \\nabla f(x_{k}) - B_k g_k) g_k^T }{(g_k, g_k)}$\n",
    "\n",
    "Подставляя найденное решение в $B_{k+1}^T \\Delta x_k = g_k$ и заметив, что $B_{k+1}^T \\Delta x_k \\neq 0$, заключаем,\n",
    "что $\\beta_k B_{k+1}^T \\Delta x_k = g_k$, и $\\beta_k = \\pm \\sqrt{\\frac{(\\nabla f(x_{k+1}) - \\nabla f(x_{k}), \\Delta x_k)}{(H_k \\Delta x_k, \\Delta x_k)}}$\n",
    "\n",
    "Отсюда выводится формула для обновления ранга 2 (дана в начале параграфа)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Докажите, что градиент и гессиан в безусловной задаче нелинейных наименьших квадратов имеет вид, приведённый в лекции.\n",
    "\n",
    "2) Почему поиск решения системы линейных алгебраических уравнений с положительно определённой симметричной матрицей эквивалентен поиску минимума приведённого в лекции функционала энергии?\n",
    "\n",
    "\n",
    "3) Вычислите оптимальное значение шага в методе сопряжённых градиентов для решения системы линейных уравнений..\n",
    "\n",
    "\n",
    "4) Реализуйте метод сопряжённых градиентов с ускорением андерсона. Как количество регрессоров влияет на сходимость?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Список материалов\n",
    "\n",
    "(1) http://www.numerical.rl.ac.uk/people/nimg/course/lectures/raphael/lectures/\n",
    "\n",
    "(2) http://www.ing.unitn.it/~bertolaz/2-teaching/2011-2012/AA-2011-2012-OPTIM/lezioni/slides-TR.pdf\n",
    "\n",
    "(3) http://www.imm.dtu.dk/pubdb/views/edoc_download.php/3215/pdf/imm3215.pdf\n",
    "\n",
    "(4) http://www.duke.edu/~hpgavin/lm.pdf\n",
    "\n",
    "(x) P.P. Pratapa, P. Suryanarayana, J.E. Pask. Anderson acceleration of the Jacobi iterative method: an efficient alternative to Krylov methods for large, sparse linear systems. JCP, 2016. URL: https://e-reports-ext.llnl.gov/pdf/824645.pdf\n",
    "\n",
    "(xx) H.F. Walker, P. Ni. Anderson acceleration for fixed-point iterations. SIAM J.Numer.Anal., \n",
    "V.49(4):1715–1735, 2011. URL: https://users.wpi.edu/~walker/Papers/Walker-Ni,SINUM,V49,1715-1735.pdf\n",
    "\n",
    "(u) Y. Saad. Iterative methods for sparse linear systems. 2nd edition, SIAM, 2003. URL: http://www-users.cs.umn.edu/~saad/IterMethBook_2ndEd.pdf\n",
    "\n",
    "http://www4.ncsu.edu/~ctk/TALKS/Anderson.pdf\n",
    "\n",
    "[pp] https://www.casl.gov/sites/default/files/docs/CASL-U-2014-0226-000.pdf\n",
    "\n",
    "https://en.wikipedia.org/wiki/Woodbury_matrix_identity\n",
    "\n",
    "http://premolab.ru/pub_files/pub5/MnexoB89z7.pdf\n",
    "\n",
    "http://www.numerical.rl.ac.uk/people/nimg/course/lectures/raphael/lectures/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
