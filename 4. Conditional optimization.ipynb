{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Курс \"Компонентные модели\"\n",
    "\n",
    "## Автор: Харюк Павел, аспирант факультета ВМК МГУ имени М.В. Ломоносова\n",
    "### Составлено: 2017-2018 гг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Занятие 4. Методы условной оптимизации\n",
    "\n",
    "Занятия, посвящённые методам оптимизации, во многом опираются на курс Ника Гоулда [\"Непрерывная оптимизация\"](http://www.numerical.rl.ac.uk/people/nimg/course/lectures/raphael/lectures/courseoutline.pdf) (Оксфорд):\n",
    "http://www.numerical.rl.ac.uk/people/nimg/course/lectures/raphael/lectures/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выпуклая оптимизация\n",
    "\n",
    "В условной оптимизации значения переменных ограничены условиями. Это означает, что условия задают некоторое подмножество $Q \\subset \\mathbb{R}^{n}$. Одним из важных классов задач является выпуклая оптимизация, где множество $Q$ является выпуклым, и оптимизируемый на нём функционал также выпуклый.\n",
    "\n",
    "**Определение.** Множество $Q \\subset \\mathbb{R}^{n}$  является выпуклым, если для $\\forall x, y \\in Q$ точка $(\\lambda x + (1- \\lambda) y) \\in Q$,  $\\forall \\lambda \\in [0, 1]$.\n",
    "\n",
    "То есть, если провести отрезок между любыми двумя точками выпуклого множества, то все точки этого отрезка также будут принадлежать множеству.\n",
    "\n",
    "**Примеры выпуклых множеств**. \n",
    "- пустое множество;\n",
    "- полупространства, $\\{x \\big| \\, (x, a) \\geq 0\\}$;\n",
    "- многогранники, $\\{ x \\big| \\, Ax \\geq b\\}$;\n",
    "- открытые шары, $B_{\\varepsilon} (u) = \\{ x \\big| \\, \\| x - u\\| < \\rho\\}$;\n",
    "- эллипсоиды, $\\{ x \\big| \\, (Ax, x) \\leq b, \\, A > 0\\}$;\n",
    "- афинные подпространства, $\\{ x \\big| \\, (a, x) = b\\}$.\n",
    "\n",
    "Важно, что линейное отображение сохраняет выпуклость множеств:\n",
    "если $Q_1, Q_2 \\subseteq \\mathbb{R}^{n}$ - выпуклые, $\\alpha \\in \\mathbb{R}$, $\\phi: \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$ - произвольное линейное отображение, тогда выпуклыми будут следующие множества:\n",
    "1. $Q_1 + Q_2 = \\{x+y \\big| \\, x \\in Q_1, \\, y \\in Q_2\\}$;\n",
    "2. $\\alpha Q_1 = \\{\\alpha x \\big| \\, x \\in Q_1\\}$;\n",
    "3. $\\phi(Q_1) = \\{\\phi(x) \\big| \\, x \\in Q_1\\}$;\n",
    "4. $Q_1 \\cap Q_2$.\n",
    "\n",
    "Выпуклые задачи обладают дифференциальными свойствами. Приводим их без доказательства (доказательства можно найти в [o])\n",
    "\n",
    "$f: Q \\to \\mathbb{R}$, $Q \\subset \\mathbb{R}^n$ - открытое выпуклое подмножество, тогда:\n",
    "\n",
    "1. Для выпуклой $f(x)$ локальный минимум совпадает с глобальным;\n",
    "2. Если $f(x)$ непрерывно дифференцируем на $Q$, тогда выпуклость функционала $f(x)$ эквивалентна условию\n",
    "$f(y) \\geq f(x) + \\nabla f(x) (y-x)$ $\\forall x, y \\in Q$;\n",
    "3. Если $f(x)$ выпукла и $\\nabla f(x^*) = 0$, то $x^*$ - точка глобального минимума. Условие является необходимым и достаточным, если $Q \\equiv \\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общая задача условной оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем виде в задаче условной оптимизации дополнительно к оптимизации исходного целевого функционала $f(x)$ добавляется несколько условий, записанных в виде равенств и неравенств:\n",
    "\n",
    "$$\\begin{array}{rl}\n",
    "\\min & f(x) \\\\\n",
    "s.t. & g_i(x) = 0, \\quad i=\\overline{1, I} \\\\\n",
    "     & h_j(x) \\geq 0, \\quad j=\\overline{1, J}\\\\\n",
    "\\end{array}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод множителей Лагранжа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "рассмотрим условную задачу оптимизации с условиями только типа равенства. Классический метод, в котором соответствие каждому условию контролируется специальной переменной, - метод множителей Лагранжа:\n",
    "\n",
    "$$\\begin{array}{rl}\n",
    "\\min & f(x) \\\\\n",
    "s.t. & g_i(x) = 0, \\quad i=\\overline{1, I} \\\\\n",
    "\\end{array} \\iff\n",
    "\\begin{array}{l}\n",
    "\\min \\mathcal{L}(x, \\lambda), \\\\\n",
    "\\mathcal{L}(x, \\lambda) = f(x) + \\big( \\lambda, g(x) \\big),\\\\\n",
    "\\end{array} \\quad\n",
    "\\begin{array}{l}\n",
    "    \\lambda = \\begin{bmatrix} \\lambda_1, & \\ldots, & \\lambda_I \\end{bmatrix}^T \\\\\n",
    "    g(x) = \\begin{bmatrix} g_1(x), & \\ldots, & g_I(x) \\end{bmatrix}^T \\\\\n",
    "    A = \\begin{bmatrix} A_1, & \\ldots, & A_I \\end{bmatrix}^T \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "Таким образом, исходная задача сводится к безусловной задаче оптимизации. Далее для нового функционала ищутся стационарные точки из векторного уравнения $\\nabla \\mathcal{L}(x, \\lambda) = 0$, которому соответсвует следующая система уравнений:\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\nabla f(x) + \\sum\\limits_{i=1}^I \\lambda_i \\nabla g_i(x) = 0\\\\\n",
    "g(x) = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Стационарные точки можно найти методом Ньютона:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "H & \n",
    "\\begin{array}{c}\n",
    "\\nabla g_1 & \\ldots & \\nabla g_I \\\\\n",
    "\\end{array} \\\\\n",
    "\\begin{array}{c}\n",
    "(\\nabla g_1)^T\\\\\n",
    "\\vdots & \\\\\n",
    "(\\nabla g_I)^T\\\\\n",
    "\\end{array} & {\\large O} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\Delta x \\\\\n",
    "\\Delta \\lambda_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\Delta \\lambda_I \\\\\n",
    "\\end{bmatrix} = \n",
    "-\\begin{bmatrix}\n",
    "\\nabla f + \\sum_{i=1}^I \\lambda_i \\nabla g_i \\\\\n",
    "g_1 \\\\\n",
    "\\vdots \\\\\n",
    "g_I \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Условия типа неравенств учитываются с помощью т.н. **дополняющей нежёсткости**. Вновь составляется функция Лагранжа, $\\mathcal{L}(x, \\mu) = f(x) + \\big( \\mu, h(x) \\big)$, для неё выписывается уравнение стационарных точек, которая справедлива для случая $h_j(x) = 0$, $j=\\overline{1, J}$. В этом случае $\\mu_j \\geq 0$. Если же условие равенства не выполнено, то его игнорируют выбором $\\mu_j = 0$, что формулируется в виде $\\mu_j \\frac{\\partial \\mathcal{L}(x, \\mu)}{\\partial \\mu_j} = 0$, условия дополняющей нежёсткости. Таким образом, задача поиска стационарных точек условной задачи оптимизации с условиями типа неравенств принимает вид\n",
    "\n",
    "$$\\begin{cases}\n",
    "    \\frac{\\partial \\mathcal{L}(x, \\mu)}{\\partial x} = \\nabla f(x) + \\sum_{j=1}^{J} \\mu_j (h_j(x) ) = 0, \\\\\n",
    "    \\frac{\\partial \\mathcal{L}(x, \\mu)}{\\partial \\mu_j} = \\mu_j (h_j(x) ) = 0, \\\\\n",
    "    h_j(x) \\geq 0, \\\\\n",
    "    \\mu_j \\geq 0, \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Далее для обозначения $\\mu_j$ будем использовать $\\lambda_j$.\n",
    "\n",
    "**Необходимое условие оптимальности первого порядка:**  Если $x^*$ является точкой локального минимума условной задачи оптимизации и выполнено условие линейной независимости градиентов $\\nabla g_i(x^*)$ и $\\nabla h_j(x^*)$ (для тех условий типа неравенств, которые обращаются в ноль: $h_j(x^*) = 0$), то найдётся такое $\\lambda^*$, что пара $(x^*, \\lambda^*)$ являются решением системы:\n",
    "$$\\begin{cases}\n",
    "\\frac{\\partial \\mathcal{L}(x^*, \\lambda^*)}{\\partial x} = 0, \\\\\n",
    "\\lambda_j \\geq 0, & j = \\overline{1, J} \\\\\n",
    "\\lambda_j^* h_j(x^*) = 0, &  j = \\overline{1, J} \\\\\n",
    "h_j(x^*) \\geq 0, &  j = \\overline{1, J} \\\\\n",
    "g_i(x^*) = 0, & i = \\overline{1, I} \\\\\n",
    "\\end{cases}\\quad\\quad (*)$$\n",
    "\n",
    "Условия (*) называют также условиями Каруша-Куна-Таккера.\n",
    "\n",
    "\n",
    "**Необходимое условие оптимальности второго порядка:**  Если $x^*$ является точкой локального минимума условной задачи оптимизации, выполнено условие линейной независимости градиентов $\\nabla g_i(x^*)$ и $\\nabla h_j(x^*)$ (для тех условий типа неравенств, которые обращаются в ноль: $h_j(x^*) = 0$), выполнено необходиомое условие оптимальности первого порядка с соответсвующим $\\lambda^*$, тогда для любых $d$ из окрестности $x^*$, удовлетворяющих условиям\n",
    "$$\\begin{cases}\n",
    "\\lambda_i^*(\\nabla g_i(x^*), d) = 0, & i = \\overline{1, I} \\\\\n",
    "\\lambda_j^*(\\nabla h_j(x^*), d) = 0, & j = \\overline{1, J} \\\\\n",
    "\\end{cases}$$\n",
    "справедливо неравенство\n",
    "$$\\Big(\\frac{\\partial^2 \\mathcal{L}(x^*, \\lambda^*)}{\\partial x^2} d, d\\Big) \\geq 0$$\n",
    "\n",
    "**Достаточное условие оптимальности второго порядка:** Пусть $x^*$ и $\\lambda^*$ такие, что выполнены условия ($(*)$) и условия линейной независимости градиентов (как в неоьходимом условии оптимальности первого порядка), тогда для того, чтобы $x^*$ была точкой локального минимуму, достаточно, чтобы\n",
    "для любых $d$ из окрестности $x^*$, удовлетворяющих условиям\n",
    "$$\\begin{cases}\n",
    "\\lambda_i^*(\\nabla g_i(x^*), d) = 0, & i = \\overline{1, I} \\\\\n",
    "\\lambda_j^*(\\nabla h_j(x^*), d) = 0, & j = \\overline{1, J} \\\\\n",
    "\\end{cases}$$\n",
    "выполнялось неравенство\n",
    "$$\\Big(\\frac{\\partial^2 \\mathcal{L}(x^*, \\lambda^*)}{\\partial x^2} d, d\\Big) > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод штрафов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В метод штрафов условная задача оптимизации сводится к последовательности безусловных задач. Это происходит следующим образом: к основному функционалу добавляются слагаемые с некоторыми весами, штрафующие решение задачи за выход из области допустимости. Примером штрафной функции яфляется функция вида\n",
    "$$\\phi_{h_j}(x) = \\min(0, h_j(x))$$\n",
    "\n",
    "$$\\begin{array}{rl}\n",
    "\\min\\limits_x & f(x) \\\\\n",
    "\\text{s.t.} & h_j(x) \\geq 0. \\quad j = \\overline{1, J} \\\\\n",
    "\\end{array} \\iff\n",
    "\\min\\limits_x \\quad f(x) + \\frac{1}{2 \\mu_k} \\sum\\limits_j \\phi_{h_j}^2 (x), \\quad \\mu_k > 0 $$\n",
    "\n",
    "Параметр $\\mu_k$ последовательно уменьшается.\n",
    "\n",
    "**Сходимость метода штрафов**\n",
    "Пусть $f, h_j \\in C^1$, $x^*$ - точка накопления последовательности решений исходной задачи методом штрафов при уменьшающемся параметре $\\mu_k$, пусть некоторая подпоследовательность $\\{x_{k_l}\\}$ этой последовательности сходится к $x^*$. Пусть все градиенты функций условий $\\nabla h_j(x) : h_j(x) \\leq 0$ линейно независимы. Пусть $\\lambda_{j; k} = -\\frac{\\phi_{h_j}(x_{k+1})}{\\mu_k}$, тогда справедливы утверждения:\n",
    "\n",
    "1. $x^*$ удовлетворяет условиям оптимизации;\n",
    "2. выполнено условие линейной независимости градиентов  $\\nabla h_j(x^∗) : h_j(x^*) = 0$\n",
    "3. существует предел $\\lim_{l\\to \\infty} \\lambda_{;k_l} = \\lambda^*$ \n",
    "4. $(x^*, \\lambda^*)$ удовлетворяет необходимым условиям оптимальности первого порядка.\n",
    "\n",
    "Метод можно применять и в случае наличия условий типа равенств."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод расширенного Лагранжиана"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(англ. Augmented Lagrange method)\n",
    "\n",
    "Как и в классическом методе множителей Лагранжа, исходная условная задача заменяется на безусловную через введение множителей Лагранжа. Отличие состоит в том, что в функционал включается штраф за выход приближения из области допустимости:\n",
    "\n",
    "$$\\begin{array}{lcl}\n",
    "\\begin{array}{rl}\n",
    "\\min\\limits_{x} & f(x) \\\\\n",
    "\\text{s.t.} & g_i(x) = 0,  & i=\\overline{1, I} \\\\\n",
    " & h_j(x) \\geq 0,  & j=\\overline{1, J} \\\\\n",
    "\\end{array} & \n",
    "\\iff & \n",
    "\\begin{array}{l}\n",
    "\\min\\limits_{x, \\lambda} \\quad f(x) + \\sum_i \\lambda_i g_i(x) + \\sum_j \\lambda_j h_j(x) + \\frac{1}{2 \\mu} \\Big(\\sum_i g_i^2 (x) + \\sum_j \\phi_{h_j}^2 (x)\\Big) = \\\\\n",
    "= f(x) + \\sum_i \\Big(\\frac{g_i (x)}{2 \\mu} + \\lambda_i \\Big) g_i(x) + \\sum_j \\Big(\\frac{\\phi_{h_j} (x)}{2 \\mu} + \\lambda_j \\Big) h_j(x)\n",
    "\\end{array}\n",
    "\\end{array}$$\n",
    "\n",
    "Решение обновляется следующим образом:\n",
    "$$x_{k+1} = \\arg \\min_x \\mathcal{L}(x, \\lambda_{k}; \\mu_k), \\\\\n",
    "(\\lambda_{i})_{k+1} = \\max \\Big(0, \\quad (\\lambda_{i})_k + \\frac{g_i (x_{k+1})}{\\mu_k} \\Big), \\\\\n",
    "(\\lambda_{j})_{k+1} = \\Big( 0, \\quad (\\lambda_{j})_k + \\frac{\\phi_{h_j} (x_{k+1})}{\\mu_k} \\Big), \\\\\n",
    "0 < \\mu_{k+1} < \\mu_k\n",
    "$$\n",
    "\n",
    "В отличие о метода штрафов, метод дополненного (расширенного) Лагранжиана не требует сходимости к нулю от $\\mu_k$ для получения локально оптимального решения.\n",
    "\n",
    "**Сходимость метода**. Пусть $x^*$ - точка локального минимума исходной задачи, в которой выполнено условие линейной независимости градиентов $\\nabla g_i(x^*)$, $\\nabla h_j(x^∗) : h_j(x^*) = 0$; выполнены необходимые условия оптимальности первого и второго порядка для некоторого $\\lambda^*$. Тогда существует такая строго положительная константа $\\mu^* > 0$, что $x^*$ является точкой локального минимума задачи\n",
    "$$\\min_x \\mathcal{L}(x, \\lambda^*; \\mu), \\quad \\forall \\mu \\in (0, \\mu^*].$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Барьерный метод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В барьерных методах\n",
    "\n",
    "$$\\min\\limits_{x \\in D \\subset \\mathbb{R}^n} f(x) \\iff \\min\\limits_{x \\in \\mathbb{R}^n} f(x) + B(x), \\quad\n",
    "B(x) \\to +\\infty \\text{ при } x \\to \\partial D$$\n",
    "\n",
    "Важно, что начальное приближение выбирается из области допустимости. Отсюда следует второе название - методы внутренней точки. Влияние дополнительного слагаемого на решение можно уменьшить, выбирая для $B(x)$ на каждой итерации последовательность множителей $\\alpha_k \\overset{k\\to\\infty}{\\to} 0$\n",
    "\n",
    "Одним из распространённых варинтов барьерной функции - логарифмическая. Например, условие $x > a$ можно включить следующим образом:\n",
    "$$\\min\\limits_{x \\in \\mathbb{R}^n_{> a} \\subset \\mathbb{R}^n} f(x) \\iff \\min\\limits_{x \\in \\mathbb{R}^n} f(x) - \\log(x-a), \\quad x_0 \\in \\mathbb{R}^n_{> a}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проекционный метод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В проекционных итерационных методах каждая итерация включает в себя два шага:\n",
    "- получение следующего приближения решения безусловной оптимизации;\n",
    "- проекция приближения на допустимое множество\n",
    "\n",
    "Проекция определяется как линейное отображение $P$, для которого $P^2 = P$.\n",
    "\n",
    "Такие методы имеет смысл применять в том случае, если проектирование недорого относительно обновления решения безусловной задачи.\n",
    "\n",
    "Примеры:\n",
    "\n",
    "1. $x \\geq a$: $\\quad$ $Px = \\max(x, a)$ (проекция на неотрицательную полуось)\n",
    "2. $Px \\in L(Y[:, 1], \\ldots, Y[:, n])$: $\\quad$ $P x = Y(Y^T Y)^{-1} Y^T x$ (проекция на линейное пространство, наятнутое на столбцы матрицы $Y$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задания\n",
    "\n",
    "1) Докажите, что приведённые примеры выпуклых множеств на самом деле являются выпуклыми множествами.\n",
    "\n",
    "2) На предыдущем занятии было предложено реализовать алгоритм вычисления малорангового матричного разложения $A = UV^T$ с помощью задачи безусловной оптимизации (в смысле наименьших квадратов). Попробуйте добавить к ней условия:\n",
    "\n",
    "2.1) на элементы матрицы $U$: $a \\leq u_{i,j} \\leq b$;\n",
    "\n",
    "2.2) на элементы матрицы $V$: $V^T V = I$;\n",
    "\n",
    "2.3) на матричное произведение: $U V^T \\geq 0$ \n",
    "\n",
    "Реализуйте алгоритм, приведите графики сходимости решения. Насколько точно полученные решения соответствуют условиям?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Список материалов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[o] http://www.numerical.rl.ac.uk/people/nimg/course/lectures/raphael/lectures/lecture1.pdf\n",
    "\n",
    "http://www.mit.edu/~dimitrib/Constrained-Opt.pdf\n",
    "\n",
    "https://www.him.uni-bonn.de/fileadmin/him/Section6_HIM_v1.pdf\n",
    "\n",
    "https://projecteuclid.org/download/pdf_1/euclid.twjm/1500558299\n",
    "\n",
    "http://www.mcs.anl.gov/~anitescu/CLASSES/2011/LECTURES/STAT310-2011-lec10.pdf\n",
    "\n",
    "https://www.cs.jhu.edu/~svitlana/papers/non_refereed/optimization_1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
